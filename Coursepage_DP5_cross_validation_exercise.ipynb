{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "class HoltWinters:\n",
    "    \"\"\"\n",
    "    Holt-Winters model\n",
    "    # series - initial time series\n",
    "    # slen - length of a season\n",
    "    # alpha, beta, gamma - Holt-Winters model coefficients\n",
    "    # n_preds - predictions \n",
    "    \"\"\"   \n",
    "    def __init__(self, series, slen, alpha, beta, gamma, n_preds):\n",
    "        self.series = series\n",
    "        self.slen = slen\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.n_preds = n_preds\n",
    "        \n",
    "    def initial_trend(self):\n",
    "        sum = 0.0\n",
    "        for i in range(self.slen):\n",
    "            sum += float(self.series[i+self.slen] - self.series[i]) / self.slen\n",
    "        return sum / self.slen  \n",
    "    \n",
    "    def initial_seasonal_components(self):\n",
    "        seasonals = {}\n",
    "        season_averages = []\n",
    "        n_seasons = int(len(self.series)/self.slen)\n",
    "        # let's calculate season averages\n",
    "        for j in range(n_seasons):\n",
    "            # seasonal mean of streamflow\n",
    "            season_averages.append(sum(self.series[self.slen*j:self.slen*(j+1)])/float(self.slen))\n",
    "        # let's calculate initial values\n",
    "        for i in range(self.slen):\n",
    "            sum_of_vals_over_avg = 0.0\n",
    "            for j in range(n_seasons):\n",
    "                sum_of_vals_over_avg += self.series[self.slen*j+i]-season_averages[j]\n",
    "            seasonals[i] = sum_of_vals_over_avg/n_seasons\n",
    "        return seasonals   \n",
    "    def triple_exponential_smoothing(self):\n",
    "        self.result = []\n",
    "        self.Smooth = []\n",
    "        self.Season = []\n",
    "        self.Trend = []\n",
    "        seasonals = self.initial_seasonal_components()        \n",
    "        for i in range(len(self.series)+self.n_preds):\n",
    "            if i == 0: # components initialization\n",
    "                smooth = self.series[0]\n",
    "                trend = self.initial_trend()\n",
    "                self.result.append(self.series[0])\n",
    "                self.Smooth.append(smooth)\n",
    "                self.Trend.append(trend)\n",
    "                self.Season.append(seasonals[i%self.slen])                \n",
    "                continue                \n",
    "            if i >= len(self.series): # predicting\n",
    "                m = i - len(self.series) + 1\n",
    "                val = self.result[i-1]\n",
    "                last_smooth, smooth = smooth, self.alpha*(val-seasonals[i%self.slen]) + (1-self.alpha)*(smooth+trend)\n",
    "                trend = self.beta * (smooth-last_smooth) + (1-self.beta)*trend\n",
    "                seasonals[i%self.slen] = self.gamma*(val-smooth) + (1-self.gamma)*seasonals[i%self.slen]\n",
    "                self.result.append((smooth + m*trend) + seasonals[i%self.slen])                \n",
    "                # when predicting we increase uncertainty on each step\n",
    "            else:\n",
    "                val = self.series[i-1]\n",
    "                last_smooth, smooth = smooth, self.alpha*(val-seasonals[i%self.slen]) + (1-self.alpha)*(smooth+trend)\n",
    "                trend = self.beta * (smooth-last_smooth) + (1-self.beta)*trend\n",
    "                seasonals[i%self.slen] = self.gamma*(val-smooth) + (1-self.gamma)*seasonals[i%self.slen]\n",
    "                self.result.append(smooth+trend+seasonals[i%self.slen])                \n",
    "            self.Smooth.append(smooth)\n",
    "            self.Trend.append(trend)\n",
    "            self.Season.append(seasonals[i%self.slen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit \n",
    "\n",
    "# prediction horizon is the same as the size of test dataset and validation dataset\n",
    "predicts = 5\n",
    "# split dataset into (training_validation) and test dataset\n",
    "datatrain = ...\n",
    "# find out the amount of folds you can get from dataset\n",
    "n_splits =...\n",
    "\n",
    "# set the number of folds for cross-validation\n",
    "# We have size of prediction: len(data[:-testsize])/(n_splits+1)\n",
    "tscv = TimeSeriesSplit(n_splits)  \n",
    "\n",
    "# use start index to determine the length of first training dataset (how many folds)\n",
    "startindex = 0 \n",
    "for train, test in tscv.split(datatrain):\n",
    "    startindex+=1\n",
    "    if ...:\n",
    "        # training size would increase from i*len(prediction) to n_splits*len(prediction)\n",
    "        print('Training set size:')\n",
    "        print(len(train))\n",
    "        print('Validation set:')\n",
    "        print(test)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def Timeseries_CVscore(params, series, slen, n_splits, loss_function=mean_squared_error):\n",
    "    \"\"\"\n",
    "        Returns error on Cross-Validation          \n",
    "        params  - vector of parameters for optimization\n",
    "        series  - time series dataset\n",
    "        slen    - season length for Holt-Winters model\n",
    "        n_splits- number of folds within series\n",
    "    \"\"\"\n",
    "    # errors array\n",
    "    Errors = []    \n",
    "    values = series.values\n",
    "    alpha, beta, gamma = params\n",
    "    \n",
    "    # set the number of folds for cross-validation\n",
    "    tscv = TimeSeriesSplit(n_splits)  \n",
    "   \n",
    "    # iterate over folds, train model on each, forecast and calculate error\n",
    "    startindex = 0 \n",
    "    for train, test in tscv.split(values):\n",
    "        startindex+=1\n",
    "        if ...:\n",
    "            # training size would increase from i*len(prediction) to n_splits*len(prediction)\n",
    "            prediction = ...\n",
    "            validation set = ...\n",
    "            error = loss_function(...,...)\n",
    "            Errors.append(error)\n",
    "    return (mean of error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from scipy.optimize import minimize              # for function optimization\n",
    "\n",
    "# prediction horizon is the same as the size of test dataset and validation dataset\n",
    "predicts = 5\n",
    "# split dataset into (training+validation) and test dataset\n",
    "datatrain = ...\n",
    "# initializing model parameters alpha, beta and gamma\n",
    "x_iguess = [0.7, 0.1, 0.3] \n",
    "\n",
    "# split traindataset into chunks of dataset with same length as test datasize\n",
    "n_splits = ... \n",
    "\n",
    "# season length is assumed to be 13 days\n",
    "slength  = 13\n",
    "\n",
    "# Minimizing the value of loss function\n",
    "opt = minimize(...)\n",
    "\n",
    "# take optimal values\n",
    "alpha_final, beta_final, gamma_final = opt.x\n",
    "print('alpha', 'beta ', 'gamma')\n",
    "print(\"{:.3f}\".format(alpha_final), \"{:.3f}\".format(beta_final),\"{:.3f}\".format(gamma_final))\n",
    "\n",
    "# forecasting for the next (predicts) days with optimal parameters\n",
    "model = HoltWinters(...)\n",
    "model.triple_exponential_smoothing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report the error\n",
    "MSE_train = \n",
    "print('MSE of train dataset:')\n",
    "print(\"{:.2f}\".format(MSE_train))\n",
    "MSE_test = \n",
    "print('MSE of test dataset:')\n",
    "print(\"{:.2f}\".format(MSE_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result\n",
    "plt.plot(Time, Daily.discharge)\n",
    "plt.plot(Time,model.result)\n",
    "ax.set(xlabel='Date', \n",
    "       ylabel='Discharge Value (cfs)',\n",
    "       title='Wabash River at Lafayette Station 2019');\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "grid_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
